# -*- coding: utf-8 -*-
"""preprocess.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jdK1Xv6Ux4ERvkie9NH13MBUekBzemJA
"""

def remove_pattern(input_txt, pattern):
    import re
    r = re.findall(pattern, input_txt)
    for i in r:
        input_txt = re.sub(i, '', input_txt)
        
    return input_txt

def preprocessdata(df):
  #replace 4 with 1 and drop class 2
#     df.loc[df.sentiment == 4, 'sentiment'] = 1
#     df.drop(df[df.sentiment == 2].index, inplace=True)

  #removing text which has length more than 280 char(max char limit of twet)
  df['pre_clean_len'] = [len(t) for t in df.text]
  df[df.pre_clean_len <= 280]
  #df['pre_clean_len']
  import numpy as np
  #df['First Season'] = np.where(df.pre_clean_len <= 280)
  b= (df.pre_clean_len <=280) #boolean output column for hour value lying between 0 to 6 
  df=df.iloc[np.where(b)] 

  #removing hyperlinks in train data
  df['text'] = df['text'].str.replace('http\S+|www.\S+', '', case=False)
  
  #removing hyperlinks in test data
  df['text'] = df['text'].str.replace('http\S+|www.\S+', '', case=False)
  
  #removing @username in train data
  df['text'] = np.vectorize(remove_pattern)(df['text'], "@[\w]*")
  
  #1.1 Convert text to lowercase 
  df['text']=df['text'].str.lower()
  
  #Remove numbers in train data
  df.text = df.text.str.replace('\d+', '')
  
  #1.3 Remove punctuation in train data
  import string
  (str(df['text']).translate(str.maketrans("","",string.punctuation)))
  
  # remove special characters, numbers, punctuations in train data
  df['text'] = df['text'].str.replace("[%!@$^#&*?.;:+_-]", " ")
  
  #1.5 Remove stop words 
  import nltk                                
  nltk.download('stopwords')
  from nltk.corpus import stopwords
  stop = stopwords.words("english")
  
  #stop words removal from train data
  df['text']= df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
  
  #removing short words from train, dev and test data
  df['text'] = df['text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))
  
  #Remove leading and ending whitespaces from train
  df['text'] = df['text'].str.strip() 
  
  
  #Step 2 Lemmatization in train, dev and test data             # this works 
  import nltk
  nltk.download('wordnet')
  from nltk.stem.wordnet import WordNetLemmatizer
  lmtzr = WordNetLemmatizer()
  
  lem_data = df['text'].apply(lambda x: ' '.join([lmtzr.lemmatize(word,'v') for word in x.split() ]))

  #Step 3 Tokenization in train, dev and test             
  tok_data = lem_data.apply(lambda x: x.split())
  
  return lem_data, tok_data
