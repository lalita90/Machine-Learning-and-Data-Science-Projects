#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Nov 19 16:05:13 2019

@author: lalita
"""

import gym
import numpy as np
import numpy.random as rnd
import matplotlib.pyplot as plt
import random

#%matplotlib inline
env=gym.make('FrozenLake-v0')
env.render()

#%%
s = env.reset()

done = False
R = []
while not done:
    a = env.action_space.sample()
        
    s, r, done, info = env.step(a)
    R.append(r)
    print('state:',s,'action:',a,'reward',r,'done or not:',done)
    #state: 4 action: 1 reward 0.0 done or not: False (Down)
    #Always starting in state S, then taking action 1 (down), going to state 4, getting a
    #reward of 0, indicating if the episode is terminated or not.
    #"the ice is slippery, so you won't always move in the direction you intend."
#     env.render()

    
# env.render()
print(R)
#%%
#Create random policy function
def create_random_policy(env):
     policy = {}
     for key in range(0, env.observation_space.n):
          current_end = 0
          p = {}
          for action in range(0, env.action_space.n):
               p[action] = 1 / env.action_space.n
          policy[key] = p
     return policy

randompolicy = create_random_policy(env)
print(randompolicy)
#%%
#Create state action dictionary
def create_state_action_dictionary(env, policy):
    Q = {}
    for key in policy.keys():
         Q[key] = {a: [0.0] for a in range(0, env.action_space.n)}
    return Q
Q_test = create_state_action_dictionary(env,randompolicy)
#%%
#function to play episode
def run_game(env, policy, display=True):
     env.reset()
     episode = []
     finished = False

     while not finished:
          s = env.env.s
          if display:
#               clear_output(True)
               env.render()
#               sleep(1)

          timestep = []
          timestep.append(s)
          n = rnd.uniform(0, sum(policy[s].values()))
          top_range = 0
          for prob in policy[s].items():
                 top_range += prob[1]
                 if n < top_range:
                       action = prob[0]
                       break 
          state, reward, finished, info = env.step(action)
          timestep.append(action)
          timestep.append(reward)

          episode.append(timestep)
#           env.render()

     if display:
#           clear_output(True)
          env.render()
#          sleep(1)
     return episode

test_episode = run_game(env,randompolicy,False)
test_episode
#%%
#function to test policy
def test_policy(policy, env):
      wins = 0
      r = 100
      for i in range(r):
            w = run_game(env, policy, display=False)[-1][-1]
            if w == 1:
                  wins += 1
      return wins / r
#%%
#Monte carlo on policy function. This is the main code.

def monte_carlo_e_soft(env,gamma, episodes=100, policy=None, epsilon=0.01):
    if not policy:
        policy = create_random_policy(env)  # Create an empty dictionary to store state action values    
    Q = create_state_action_dictionary(env, policy) # Empty dictionary for storing rewards for each state-action pair
    returns = {} # 3.
    G_list = []
    for _ in range(episodes): # Looping through episodes
        G = 0 # Store cumulative reward in G (initialized at 0)
        G_with_Gamma = 0
        episode = run_game(env=env, policy=policy, display=False) # Store state, action and value respectively 
        
        # for loop through reversed indices of episode array. 
        # The logic behind it being reversed is that the eventual reward would be at the end. 
        # So we have to go back from the last timestep to the first one propagating result from the future.
        
        for i in reversed(range(0, len(episode))):   #This needs modification
            s_t, a_t, r_t = episode[i] 
            state_action = (s_t, a_t)
            G_with_Gamma = r_t + gamma*G # Increment total reward by reward on current timestep
            G += r_t
            
            if not state_action in [(x[0], x[1]) for x in episode[0:i]]: # 
                if returns.get(state_action):
                    returns[state_action].append(G_with_Gamma)
                else:
                    returns[state_action] = [G_with_Gamma]   
                    
                Q[s_t][a_t] = np.array(sum(returns[state_action]) / len(returns[state_action])) # Average reward across episodes
                
                Q_list = list(map(lambda x: x[1], Q[s_t].items())) # Finding the action with maximum value
                Q_array = np.array(Q_list)
                indices = [i for i, x in enumerate(Q_list) if x == np.max(Q_array)]
                max_Q = random.choice(indices)
                
                A_star = max_Q # 14.
                
                for a in policy[s_t].items(): # Update action probability for s_t in policy
                    if a[0] == A_star:
                        policy[s_t][a[0]] = 1 - epsilon + (epsilon / abs(sum(policy[s_t].values())))
                    else:
                        policy[s_t][a[0]] = (epsilon / abs(sum(policy[s_t].values())))
        G_list.append(G)
        
    return policy, G_list