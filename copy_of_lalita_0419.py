# -*- coding: utf-8 -*-
"""Copy of Lalita_0419.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/134L5VjSKNEYqVnRM_yp0GJcCTXd-bXyq
"""

from google.colab import drive
drive.mount("/content/drive")

!ls 'drive/Team Drives/CSCE 633 Project Group/Dataset/Sentiment140'

root = "drive/Team Drives/CSCE 633 Project Group/Dataset/Sentiment140"

#!echo "tmpfs /dev/shm tmpfs defaults,size=100g 0 0" >> ../../../etc/fstab

#!mount -o remount /dev/shm

import matplotlib.pyplot as plt
import pandas as pd
cols = ['sentiment','id','date','query_string','user','text']
df = pd.read_csv(root + '/train.csv', engine='python',encoding = "ISO-8859-1",names=cols)
# Dataset is now stored in a Pandas Dataframe\

#df.sentiment.value_counts()  #count the number of "0" and "4"

#reading test data
cols = ['sentiment','id','date','query_string','user','text']
test = pd.read_csv(root + '/test.csv', engine='python',encoding = "ISO-8859-1",names=cols)

#reading dev data
cols = ['sentiment','id','date','query_string','user','text']
dev = pd.read_csv(root + '/dev.csv', engine='python',encoding = "ISO-8859-1",names=cols)


normal_words =' '.join([text for text in df.text.apply(str)[df['sentiment'] == 0]])

wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)


normal_words =' '.join([text for text in df.text.apply(str)[df['sentiment'] == 4]])

wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)


import pandas as pd
#load lem data
lem_data = pd.read_pickle(root + '/lem_train.pkl')
lem_dev = pd.read_pickle(root + '/lem_dev.pkl')
lem_test = pd.read_pickle(root + '/lem_test.pkl')

#word2vec
from gensim.models import Word2Vec
model_ted = Word2Vec(sentences=tok_text, size=10000, window=5, min_count=5, workers=4, sg=0)

#checking similar words
model_ted.wv.most_similar('singer')

tok_text.shape

#checking similar words
model_ted.wv.most_similar('facebook')

Word2Vec(sentences=tok_text, size=1000, window=5, min_count=5, workers=4, sg=0)



from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
vocabulary_size = 10000
tokenizer = Tokenizer(num_words= vocabulary_size)
tokenizer.fit_on_texts(lem_data)##
sequences = tokenizer.texts_to_sequences(lem_data)
data = pad_sequences(sequences, maxlen=30)          ## 30       #check by changing to 30-40


seq_dev=tokenizer.texts_to_sequences(lem_dev)##
data_dev = pad_sequences(seq_dev,maxlen=30)


df.loc[df.sentiment == 4, 'sentiment'] = 1
test.loc[test.sentiment == 4, 'sentiment'] = 1
test.drop(test[test.sentiment == 2].index, inplace=True)
dev.loc[dev.sentiment == 4, 'sentiment'] = 1

# dev['sentiment']

## Network architecture model 
import keras
from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D
from keras.models import Sequential
from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation
from keras.layers.embeddings import Embedding
#custom_adam = keras.optimizers.Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=1e-8)
model = Sequential()
#model.add(Dense(30, activation='sigmoid'))
model.add(Embedding(20000, 100, input_length=30))
#model.add(LSTM(10, dropout=0.2))   #lstm layer should be low
model.add(LSTM(10,dropout=0.2))
model.add(Dense(128, activation='sigmoid'))
model.add(Dense(1, activation='sigmoid'))                     #sigmoid or softmax; dense!=5=2
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])   #loss binary cross: change 4 to 1

#model.fit(data, np.array(df['sentiment']), epochs=3)

## Network architecture model 2
from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D
from keras.models import Sequential
from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation
from keras.layers.embeddings import Embedding
model = Sequential()
model.add(Embedding(20000, 100, input_length=30))
model.add(LSTM(5, dropout=0.2, recurrent_dropout=0.2))   #lstm layer should be low
model.add(Dense(1, activation='sigmoid'))                     #sigmoid or softmax; dense!=5=2
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])   #loss binary cross: change 4 to 1

#model 3
from keras.layers import Dense, Flatten, GRU, Conv1D, MaxPooling1D
model = Sequential()
model.add(Embedding(20000, 100, input_length=30))
model.add(GRU(5, dropout=0.2, recurrent_dropout=0.2))   #lstm layer should be low
#model.add(Dense(128, activation='sigmoid', input_dim=100000))
model.add(Dense(1, activation='sigmoid'))                     #sigmoid or softmax; dense!=5=2
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])   #loss binary cross: change 4 to 1

#model4
from keras import Sequential
from keras.layers import Embedding, LSTM, Dense, Dropout
embedding_size=32
model=Sequential()
model.add(Embedding(vocabulary_size, embedding_size, input_length=30))
model.add(LSTM(100))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', 
             optimizer='adam', 
             metrics=['accuracy'])
print(model.summary())

#df.loc[df.sentiment == 4, 'sentiment'] = 1
# test.loc[test.sentiment == 4, 'sentiment'] = 1
# dev.loc[dev.sentiment == 4, 'sentiment'] = 1

import tensorflow as tf
import os #working but cant see where it is saved import tensorflow as tf
checkpoint_path =root+"/training_1/cp.ckpt" 
checkpoint_dir = os.path.dirname(checkpoint_path)
cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=True, verbose=1)

# model.fit_generator(generator=batch_generator_shuffle(data, df['sentiment'], 32),
#                     epochs=3, validation_data=(data_dev, dev['sentiment']),
#                     steps_per_epoch=data.shape[0]/128)

type(df['sentiment'])

import numpy as np
#model.fit(data, np.array(df['sentiment']), epochs=8,batch_size=128, validation_data=(data_dev, np.array(dev['sentiment'])))
history=model.fit(data, np.array(df['sentiment']),  
          epochs = 10, batch_size=1024,
          validation_data = (data_dev, np.array(dev['sentiment'])),
          callbacks = [cp_callback], shuffle=True)  # pass callback to training

history.history.keys()

"""Plots"""

import matplotlib.pyplot as plt
plt.plot(history.history['acc'], 'b')
plt.title('Training Accuracy vs epoch')

import matplotlib.pyplot as plt
plt.plot(history.history['loss'], 'r')
plt.title('Training Loss vs epoch')

import matplotlib.pyplot as plt
plt.plot(history.history['val_acc'], 'b')
plt.title('Validation Accuracy vs epoch')

import matplotlib.pyplot as plt
plt.plot(history.history['val_loss'], 'r')
plt.title('Validation Loss vs epoch')

my_tags = ['class0','class1']
plt.figure(figsize=(6,4))
df.sentiment.value_counts().plot(kind='bar')

#https://colab.research.google.com/github/tensorflow/models/blob/master/samples/core/tutorials/keras/save_and_restore_models.ipynb#scrollTo=jwEaj9DnTCVA
#save model output
!pip install h5py pyyaml 
import keras
# Save entire model to a HDF5 file
model.save('model2.h5')
# Recreate the exact same model, including weights and optimizer.
new_model = keras.models.load_model('model2.h5')
new_model.summary()
#loss, acc = new_model.evaluate(test_images, test_labels)
#print("Restored model, accuracy: {:5.2f}%".format(100*acc))

# test.drop(test[test.sentiment == 2].index, inplace=True)

#tokenizing test
seq_test=tokenizer.texts_to_sequences(lem_test)
data_test = pad_sequences(seq_test,maxlen=30)

model.evaluate(x=data_test, y=test['sentiment'])

#download model
#from google import files
from google.colab import files
files.download("model2.h5")
#files.download("model2.h5")

#hourly tweet analysis

#upload model
from google.colab import files
files.upload("model2.h5")

"""**Preprocess and TFID**"""